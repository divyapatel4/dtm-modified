\section{Dynamic Topic Model (DTM)}


\subsection{Formato dos dados de entrada}

A ferramenta \srccode{dtm} requer, no mínimo, dois arquivos de entrada:
um para a descrição de cada documento e seus respectivos termos e outro
para identificar as fatias de tempo a serem analisadas.

O primeiro arquivo, geralmente definido com o nome \srccode{???-mult.dat},
contém M linhas, sendo M a quantidade de documentos a serem analisados.
Os documentos devem ser ordenados pela data, em ordem crescente.
Cada linha descreve um documento, os seus termos e a quantidade de cada
termo no documento, de acordo com o seguinte formato:

\begin{lstlisting}
unique_word_count index1:count1 index2:count2 ... indexn:counnt
\end{lstlisting}

Não existe um identificador para o documento: o número da linha é utilizado
para esse fim. Os termos que compõe o documento também não são declarados
de forma textual: deve-se adotar um identificador numérico para cada termo.
Esse identificador deve ser único para o mesmo termo em relação a todos os
documentos (veja isto como uma otimização: é mais rápido processar um número
do que uma palavra). A contagem corresponde a frequência absoluta do termo
no documento da linha atual. Finalmente, o primeiro elemento da linha indica
o tamanho do vocabulário necessário para descrever o documento (veja isto
como um facilitador para ler os dados restantes da linha).

Por exemplo, considere os documentos da \cref{}. A definição deles no formato
DTM é apresentada na \cref{}. Observe que, para o segundo documento, alguns
termos que já tinham sido utilizados para especificar o primeiro documento,
apareceram. Logo, o índice que identifica o termo é o mesmo (\eg{], 3, 9 e 14},
embora a contagem seja particular a cada documento (\eg{}, o termo 3 apareceu
3 vezes no primeiro documento e apenas 1 vez no segundo documento).

\begin{figure}
\begin{itemize}
	\item Documento 1~\cite{Kulesza-etal2007}:
	\\``The development of collaborative and multimedia systems is a
	complex task and one of the key challenges is to promote the reuse and
	integration of those two software categories in the same environment.''

	\item Documento 2~\cite{Bezerra-Wainer2006}:
	``This work shows a model to detect a set of anomalous traces in a log
	generated by a business process management system.''
\end{itemize}	

\begin{lstlisting}
24 1:4 2:1 3:3 4:1 5:3 6:1 7:1 8:2 9:1 10:1 11:1 12:1 13:1 14:1 15:1 16:1 17:1 18:1 19:1 20:1 21:1 22:1 23:1 24:1
17 3:1 9:4 14:1 22:1 25:1 26:1 27:1 28:1 29:1 30:1 31:1 32:1 33:1 34:1 35:1 36:1 37:1
\end{lstlisting}

\begin{tabular}
1. the
2. development
3. of
4. collaborative
5. and
6. multimedia
7. systems
8 is
9 a
10 complex
11 task 
12 one
13 challenges
14 to
15 promote
16 reuse
17 integration
18 those
19 two
20 software
21 categories
22 in
23 same
24 environment
25 this
26 work
27 shows
28 model
29 detect
30 set
31 log
32 generated
33 by
34 business
35 process
36 management
37 system
\end{tabular}
\end{figure}

Para facilitar a posterior análise dos dados, é necessário guardar a
informação de qual termo corresponde a cada identificador de termo e 
de qual documento corresponde a cada identificador de documento.
Para o primeiro caso, deve-se criar um arquivo \srccode{vocab} com o
vocabulário utilizado na coleção analisada. O formato deste arquivo é
bem simples: coloca-se um termo por linha, sendo que o número da linha
corresponde ao identificador do termo. Para o segundo caso, deve-se criar
um arquivo \srccode{docs} com o nome de cada documento, organizados na
mesma ordem em que foram especificados no arquivo de entrada (\srccode{-mult.dat}).
Dessa forma, os dados em \cref{} podem ser posteriormente recuperados,
facilitando a análise dos resultados pelos pesquisadores.


O segundo arquivo de entrada do DTM define as fatias de tempo que serão
analisadas. Esse arquivo, geralmente nomeado \srccode{???-seq.dat}, adota
o seguinte formato:

\begin{lstlisting}
   Number_Timestamps
   number_docs_time_1
   ...
   number_docs_time_i
   ...
   number_docs_time_NumberTimestamps
\end{lstlisting}

A primeira linha determina a quantidade de fatias de tempo a serem analisadas.
As linhas seguintes especificam quantos documentos fazem parte de cada fatia,
em ordem crescente. Esses documentos são obtidos em sequência, do início para
o fim, do arquivo que descreve a coleção de dados a serem analisadas. Como
aquele arquivo descreve um documento por linha, serão utilizados as $M_1$
primeiras linhas para a primeira fatia de tempo, as $M_2$ linhas seguintes para
a segunda fatia de tempo e assim por diante.

Por exemplo, observando-se o \cref{}, para a fatia de tempo 1, definida na
linha 2, serão considerados 15 documentos




\subsection{Configuração}

\begin
  Flags from data.c:
    -influence_flat_years (How many years is the influence nonzero?If
      nonpositive, a lognormal distribution is used.) type: int32 default: -1
    -influence_mean_years (How many years is the mean number of citations?)
      type: double default: 20
    -influence_stdev_years (How many years is the stdev number of citations?)
      type: double default: 15
    -max_number_time_points (Used for the influence window.) type: int32
      default: 200
    -resolution (The resolution.  Used to determine how far out the beta mean
      should be.) type: double default: 1
    -sigma_c (c stdev.) type: double default: 0.050000000000000003
    -sigma_cv (Variational c stdev.) type: double
      default: 9.9999999999999995e-07
    -sigma_d (If true, use the new phi calculation.) type: double
      default: 0.050000000000000003
    -sigma_l (If true, use the new phi calculation.) type: double
      default: 0.050000000000000003
    -time_resolution (This is the number of years per time slice.) type: double
      default: 0.5

  Flags from gsl-wrappers.c:
    -rng_seed (Specifies the random seed.  If 0, seeds pseudo-randomly.)
      type: int64 default: 0

  Flags from lda-seq.c:
    -fix_topics (Fix a set of this many topics. This amounts to fixing these
      topics' variance at 1e-10.) type: int32 default: 0
    -forward_window (The forward window for deltas. If negative, we use a beta
      with mean 5.) type: int32 default: 1
    -lda_sequence_max_iter (The maximum number of iterations.) type: int32
      default: 20
    -lda_sequence_min_iter (The maximum number of iterations.) type: int32
      default: 1
    -normalize_docs (Describes how documents's wordcounts are considered for
      finding influence. Options are "normalize", "none", "occurrence", "log",
      or "log_norm".) type: string default: "normalize"
    -save_time (Save a specific time.  If -1, save all times.) type: int32
      default: 2147483647

  Flags from lda.c:
    -lambda_convergence (Specifies the level of convergence required for lambda
      in the phi updates.) type: double default: 0.01

  Flags from main.c:
    -alpha () type: double default: -10
    -corpus_prefix (The function to perform. Can be dtm or dim.) type: string
      default: ""
    -end () type: int32 default: -1
    -heldout_corpus_prefix () type: string default: ""
    -heldout_time (A time up to (but not including) which we wish to train, and
      at which we wish to test.) type: int32 default: -1
    -initialize_lda (If true, initialize the model with lda.) type: bool
      default: false
    -lda_max_em_iter () type: int32 default: 20
    -lda_model_prefix (The name of a fit model to be used for testing
      likelihood.  Appending "info.dat" to this should give the name of the
      file.) type: string default: ""
    -mode (The function to perform. Can be fit, est, or time.) type: string
      default: "fit"
    -model (The function to perform. Can be dtm or dim.) type: string
      default: "dtm"
    -ntopics () type: double default: -1
    -outname () type: string default: ""
    -output_table () type: string default: ""
    -params_file (A file containing parameters for this run.) type: string
      default: "settings.txt"
    -start () type: int32 default: -1
    -top_chain_var () type: double default: 0.0050000000000000001
    -top_obs_var () type: double default: 0.5





\subsection{Running}

This progam takes as input a collection of text documents and creates
as output a list of topics over time, a description of each document
as a mixture of these topics, and (possibly) a measure of how
"influential" each document is, based on its language.

We have provided an example dataset, instructions for formatting input
data and processing output files, and example command lines for
running this software in the file dtm/sample.sh.


\subsubsection{Topic estimation}

./main \
  --ntopics=20 \
  --mode=fit \
  --rng_seed=0 \
  --initialize_lda=true \
  --corpus_prefix=example/test \
  --outname=example/model_run \
  --top_chain_var=0.005 \
  --alpha=0.01 \
  --lda_sequence_min_iter=6 \
  --lda_sequence_max_iter=20 \
  --lda_max_em_iter=10


\subsubsection{Topic inference}

./main \
    --mode=fit \
    --rng_seed=0 \
    --model=fixed \
    --initialize_lda=true \
    --corpus_prefix=example/test \
    --outname=example/output \
    --time_resolution=2 \
    --influence_flat_years=5 \
    --top_obs_var=0.5 \
    --top_chain_var=0.005 \
    --sigma_d=0.0001 \
    --sigma_l=0.0001 \
    --alpha=0.01 \
    --lda_sequence_min_iter=6 \
    --lda_sequence_max_iter=20 \
    --save_time=-1 \
    --ntopics=10 \
    --lda_max_em_iter=10



\subsection{Resultado}

O programa \srccode{dtm} cria os seguintes arquivos:

\begin{itemize}
	\item \srccode{topic-???-var-e-log-prob.dat}: a distribuição das
	palavras (e-betas) para o tópico ??? para cada período analisado.

	Os dados contidos no arquivo estão no formato \foreign{row-major},
	ou seja, as linhas da tabela estão armazenadas uma após a outra.
	Cada linha, por sua vez, possui uma quantidade de colunas equivalente
	à quantidade de frações de tempo analisadas. Por exemplo, se foram
	analisadas 10 fatias de tempo, o comando para ler os dados relativos
	ao tópico 2 seria, em R:

	\begin{lstlisting}[language=R]
	a = scan("topic-002-var-e-log-prob.dat")
	b = matrix(a, ncol=10, byrow=TRUE)
	\end{lstlisting}

	Cada célula da matriz possui o logaritmo natural da probabilidade do
	termo M (sendo M o número identificador do termo, tal como definido
	no vocabulário e na matriz de entrada), o qual está na linha M da
	matriz, em relação ao tópico identificado pelo número N, o qual
	corresponde à coluna da matriz. Por exemplo, para obter a
	probabilidade do termo 100 para a fatia de tempo 3, o comando seria:

	\begin{lstlisting}[language=R]
	exp(b[100, 3])
	\end{lstlisting}


	\item \srccode{gam.dat}: Armazena os parâmetros do Dirichlet variacional
	para cada documento. 


	Divide these by the sum for each document to get expected topic mixtures.
	\begin{lstlisting}[language=R}
	a = scan("gam.dat")
	b = matrix(a, ncol=10, byrow=TRUE)
	rs = rowSums(b)
	e.theta = b / rs
	# Proportion of topic 5 in document 3:
	e.theta[3, 5]
	\end{lstlisting}


	\item[\srccode{influence_time-???}]: Armazena a influência dos documentos na
	fatia de tempo ??? para cada tópico. Cada linha do arquivo corresponde ao
	documento M, sendo tal identificador M equivalente à linha em que o documento
	em questão se encontra na matriz de entrada (-mult). Cada coluna corresponde
	a um tópico N. Por exemplo, para obter a influência do documento 2 no tópico
	5, os comandos em R seriam:

	\begin{lstlisting}[language=R]
	a = scan("influence-time-010")
	b = matrix(a, ncol=10, byrow=TRUE)
	b[2, 5]
	\end{lstlisting}
\end{description}


A análise de todos esses arquivos podem ser automatizada da seguinte forma:

\begin{lstlisting}[language=R]
# Para um tópico
data0 = scan("topic-000-var-e-log-prob.dat")
b0 = matrix(data0, ncol=10, byrow=TRUE)
write.table(b0, file="dist-topic0.csv", sep=";")


# Processa todos tópicos
# Para cada tópico, gera um arquivo com a probabilidade de cada
# termo para cada ano
# TODO: rodar exp() nos valores
topics = list()
for (i in 0:9) {
	filename = paste("topic-00", i, sep = "")
	filename = paste(filename, "-var-e-log-prob.dat", sep = "")
	data = scan(filename)
	topic = matrix(data, ncol=10, byrow=TRUE)
	filename = paste("dist-topic", i, sep = "")
	filename = paste(filename, ".csv", sep = "")
	write.table(topic, file=filename, sep=";")
}


# - gam.dat: The gammas associated with each document.  Divide these by
#  the sum for each document to get expected topic mixtures.
# Proportion of topic 5 in document 3:
# e.theta[3, 5]
a = scan("gam.dat")
b = matrix(a, ncol=10, byrow=TRUE)
rs = rowSums(b)
e.theta = b / rs
write.table(e.theta, file="documents_topics.csv", sep=";"
\end{lstlisting}


